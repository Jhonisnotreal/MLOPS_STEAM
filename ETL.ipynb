{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries, defining paths & creating functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries and paths\n",
    "\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import ast\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Paths\n",
    "\n",
    "path1 = 'Datasets/steam_games.json.gz'\n",
    "path2 = 'Datasets/user_reviews.json.gz'\n",
    "path3 = 'Datasets/users_items.json.gz'\n",
    "\n",
    "# Creating a function to read the paths\n",
    "\n",
    "def read_path(file):\n",
    "    with gzip.open(file, 'rt', encoding='utf-8') as myfile:\n",
    "        return [ast.literal_eval(line.strip()) for line in myfile]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Let's read all the datasets\n",
    "\n",
    "# Reading the dataset from 'steam_games.json.gz' ↓↓↓\n",
    "with gzip.open(path1, 'rt', encoding='utf-8') as file:\n",
    "    df_games = pd.read_json(file, lines=True)\n",
    "\n",
    "# Reading datasets \n",
    "reviews = read_path(path2)\n",
    "items = read_path(path3)\n",
    "\n",
    "# Transforming datasets to dataframes ↓:\n",
    "df_reviews = pd.DataFrame(reviews)\n",
    "df_items = pd.DataFrame(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL\n",
    "\n",
    "We are going to extract, transform and load data from datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, let's begin cleaning 'df_games'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with df_games\n",
    "\n",
    "# In the next code cell, I'm going to delete specific columns that we don't need for the tasks\n",
    "\n",
    "df_games.drop(['publisher','url','reviews_url','price','early_access','developer','specs'],axis=1,inplace=True)\n",
    "\n",
    "# I use inplace=True because I want to change the original dataframe too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, We drop rows where are only null values\n",
    "\n",
    "df_games.dropna(axis=0, how='all', inplace=True)\n",
    "\n",
    "# Also, we drop dulicates in 'id' column\n",
    "\n",
    "df_games.drop_duplicates(subset=['id'], inplace=True)\n",
    "\n",
    "# Now, let's drop values where there is no genres, tags, app_name and title, this is because it will only cause latency in the EDA process\n",
    "\n",
    "df_games.dropna(subset=['genres','tags'],how='all', inplace=True)\n",
    "df_games.dropna(subset=['app_name','title'],how='all', inplace=True)\n",
    "\n",
    "# We change the type of value in 'id' column to integer\n",
    "df_games['id'].fillna(0, inplace=True)\n",
    "df_games['id'] = df_games['id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's time to transform 'release_date' column. If it's a null value just returns 'None'\n",
    "# But if it's a value, just transform to datetime type \n",
    "\n",
    "def datetime_change(var):\n",
    "\n",
    "    if pd.isna(var):\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        return pd.to_datetime(var)\n",
    "    except ValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_games[\"release_date\"] = df_games[\"release_date\"].apply(datetime_change)\n",
    "\n",
    "df_games[\"release_year\"] = df_games[\"release_date\"].dt.year\n",
    "\n",
    "df_games['release_date'] = df_games['release_date'].astype('int64').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last but not least let's save it to a csv file\n",
    "\n",
    "df_games = df_games.to_csv('Games.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Step: Let's clean 'df_reviews'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_2 = df_reviews.explode('reviews')\n",
    "\n",
    "df_reviews_2 = pd.concat([df_reviews_2.drop(['reviews'], axis=1), df_reviews_2['reviews'].apply(pd.Series)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        None\n",
       "0        None\n",
       "0        None\n",
       "1        None\n",
       "1        None\n",
       "         ... \n",
       "25797    None\n",
       "25797    None\n",
       "25798    None\n",
       "25798    None\n",
       "25798    None\n",
       "Name: posted, Length: 59333, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews_2['posted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .dt accessor with datetimelike values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m df_reviews_2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposted\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(datetime_change)\n\u001b[1;32m----> 3\u001b[0m df_reviews_2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposted\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_reviews_2\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mposted\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[38;5;241m.\u001b[39myear\n",
      "File \u001b[1;32mc:\\Users\\Jhon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:5989\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5983\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   5984\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   5985\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   5986\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5987\u001b[0m ):\n\u001b[0;32m   5988\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 5989\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jhon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\accessor.py:224\u001b[0m, in \u001b[0;36mCachedAccessor.__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessor\n\u001b[1;32m--> 224\u001b[0m accessor_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;66;03m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;66;03m# NDFrame\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, accessor_obj)\n",
      "File \u001b[1;32mc:\\Users\\Jhon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\accessors.py:580\u001b[0m, in \u001b[0;36mCombinedDatetimelikeProperties.__new__\u001b[1;34m(cls, data)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_period_dtype(data\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PeriodProperties(data, orig)\n\u001b[1;32m--> 580\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only use .dt accessor with datetimelike values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can only use .dt accessor with datetimelike values"
     ]
    }
   ],
   "source": [
    "df_reviews_2['posted'].apply(datetime_change)\n",
    "\n",
    "df_reviews_2['posted'] = df_reviews_2['posted'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([    'user_id',    'user_url',       'funny',      'posted',\n",
       "       'last_edited',     'item_id',     'helpful',   'recommend',\n",
       "            'review',             0],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews_2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'user_url', 'reviews'], dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to delete innecesary columns\n",
    "df_reviews = df_reviews.explode('reviews')\n",
    "\n",
    "df_reviews = df_reviews.drop([\"reviews\",\"last_edited\",\"index\",\"posted\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third step: Cleaning 'df_items'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save the file again\n",
    "\n",
    "df_items_data = df_items.to_parquet(\"df_items_data.parquet\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a sample because is too big \n",
    "\n",
    "df_items_sample = df_items.sample(10000)\n",
    "df_items_sample.to_parquet(\"df_items_sample.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why did I choose Parquet? This is related to the structure of the dataset itself.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
